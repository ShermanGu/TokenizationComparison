{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "import os\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_dataset = load_dataset(\"wmt14\", \"de-en\", split=\"train[:1%]\")\n",
    "sentiment_dataset = load_dataset(\"amazon_polarity\", split=\"train[:1%]\")  # Just an example in English, replace with multilingual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_text_for_tokenizer(dataset, text_columns, sample_size=10000, translation_key=None):\n",
    "    texts = []\n",
    "    for i, example in enumerate(dataset):\n",
    "        if i >= sample_size:\n",
    "            break\n",
    "        if translation_key and translation_key in example:\n",
    "            # Extract text from a nested 'translation' dictionary\n",
    "            combined_text = \" \".join([example[translation_key][col] for col in text_columns])\n",
    "        else:\n",
    "            # Extract text directly if not nested\n",
    "            combined_text = \" \".join([example[col] for col in text_columns if col in example])\n",
    "        texts.append(combined_text)\n",
    "    return texts\n",
    "\n",
    "translation_texts = extract_text_for_tokenizer(translation_dataset, [\"de\", \"en\"], sample_size=10000, translation_key=\"translation\")\n",
    "\n",
    "# For sentiment datasets (e.g., amazon_polarity), texts might be directly in 'text' field\n",
    "# If they are top-level, call without 'translation_key'.\n",
    "sentiment_texts = extract_text_for_tokenizer(sentiment_dataset, [\"content\"], sample_size=10000)\n",
    "\n",
    "all_texts = translation_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tokenizers/bpe/vocab.json', 'tokenizers/bpe/merges.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train BPE Tokenizer (Baseline)\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "bpe_tokenizer = ByteLevelBPETokenizer()\n",
    "bpe_tokenizer.train_from_iterator(all_texts, vocab_size=32000, min_frequency=2, special_tokens=[\"<s>\", \"</s>\", \"<pad>\", \"<unk>\"])\n",
    "os.makedirs(\"tokenizers/bpe\", exist_ok=True)\n",
    "bpe_tokenizer.save_model(\"tokenizers/bpe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=sp_data/corpus.txt --model_prefix=tokenizers/sp_unigram --vocab_size=16000 --model_type=unigram --character_coverage=1.0 --unk_id=0 --pad_id=1 --bos_id=2 --eos_id=3\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: sp_data/corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: tokenizers/sp_unigram\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 16000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 2\n",
      "  eos_id: 3\n",
      "  pad_id: 1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: sp_data/corpus.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 10000 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=3227151\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=106\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 10000 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=1689126\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 103159 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 10000\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 44827\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 44827 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=28455 obj=11.4284 num_tokens=86820 num_tokens/piece=3.05113\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=23937 obj=8.9857 num_tokens=87276 num_tokens/piece=3.64607\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=17944 obj=9.00966 num_tokens=93912 num_tokens/piece=5.23362\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=17934 obj=8.9779 num_tokens=93917 num_tokens/piece=5.23681\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=17597 obj=8.9836 num_tokens=94376 num_tokens/piece=5.36319\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=17595 obj=8.98162 num_tokens=94377 num_tokens/piece=5.36385\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: tokenizers/sp_unigram.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: tokenizers/sp_unigram.vocab\n"
     ]
    }
   ],
   "source": [
    "#Train SentencePiece (Unigram) Tokenizer\n",
    "\n",
    "os.makedirs(\"sp_data\", exist_ok=True)\n",
    "with open(\"sp_data/corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in all_texts:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "sp_model_prefix = \"tokenizers/sp_unigram\"\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    f\"--input=sp_data/corpus.txt --model_prefix={sp_model_prefix} \"\n",
    "    f\"--vocab_size=16000 --model_type=unigram --character_coverage=1.0 \"\n",
    "    f\"--unk_id=0 --pad_id=1 --bos_id=2 --eos_id=3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tokenizers/wp/vocab.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WordPiece training\n",
    "\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "wp_tokenizer = BertWordPieceTokenizer(lowercase=True)\n",
    "wp_tokenizer.train_from_iterator(all_texts, vocab_size=32000, limit_alphabet=1000, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
    "os.makedirs(\"tokenizers/wp\", exist_ok=True)\n",
    "wp_tokenizer.save_model(\"tokenizers/wp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('tokenizers/wp/tokenizer_config.json',\n",
       " 'tokenizers/wp/special_tokens_map.json',\n",
       " 'tokenizers/wp/vocab.txt',\n",
       " 'tokenizers/wp/added_tokens.json',\n",
       " 'tokenizers/wp/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert all to Hugging Face format\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import T5Tokenizer, T5TokenizerFast\n",
    "\n",
    "\n",
    "bpe_tokenizer = ByteLevelBPETokenizer(\"tokenizers/bpe/vocab.json\", \"tokenizers/bpe/merges.txt\")\n",
    "# Wrap it with PreTrainedTokenizerFast\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=bpe_tokenizer,\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    cls_token=\"<cls>\",\n",
    "    sep_token=\"<sep>\",\n",
    "    mask_token=\"<mask>\"\n",
    ")\n",
    "wrapped_tokenizer.save_pretrained(\"tokenizers/bpe\")\n",
    "\n",
    "\n",
    "\n",
    "# Load as a slow tokenizer (this should work since it's a SentencePiece model)\n",
    "slow_tokenizer = T5Tokenizer(\"tokenizers/sp_unigram.model\", extra_ids=0)\n",
    "\n",
    "# Save the slow tokenizer to a directory\n",
    "slow_tokenizer.save_pretrained(\"tokenizers/sp_unigram_slow\")\n",
    "\n",
    "# Now convert slow to fast by loading it as a T5TokenizerFast\n",
    "fast_tokenizer = T5TokenizerFast.from_pretrained(\"tokenizers/sp_unigram_slow\")\n",
    "fast_tokenizer.save_pretrained(\"tokenizers/sp_unigram_hf\")\n",
    "\n",
    "\n",
    "\n",
    "wordpiece_tokenizer = BertTokenizerFast(\n",
    "    vocab_file=\"tokenizers/wp/vocab.txt\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\"\n",
    ")\n",
    "wordpiece_tokenizer.save_pretrained(\"tokenizers/wp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
